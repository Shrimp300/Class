from __future__ import print_function

#import numpy as np
import pandas as pd

# To import the classifier (Neural Networks)
import sklearn.neural_network as NN


# To measure accuracy
from sklearn import metrics

# To split data to train and test
from sklearn.model_selection import train_test_split

# To perform parameter search
#from sklearn.model_selection import GridSearchCV

# To support plots
import matplotlib.pyplot as plt
# To import the scalers
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import Binarizer

class DummyScaler:
    
    def fit(self, data):
        pass
    
    def transform(self, data):
        return data

def create_scaler_dummy():
    return DummyScaler()
    
def create_scaler_standard():
    return StandardScaler()

def create_scaler_minmax():
    return MinMaxScaler()

def crete_scaler_binarizer():
    return Binarizer()

# You can choose a scaler (just one should be uncommented):
# create_scaler = create_scaler_dummy
create_scaler = create_scaler_standard
# create_scaler = create_scaler_minmax
# create_scaler = create_scaler_binarizer



# Part 1. Breast cancer dataset (classification)

print('Load the dataset')
myfile = r"\Users\Norris\Onedrive\IEE520\project 1\Avila.csv"
data = pd.read_csv(myfile)
dataset=data.values
X=dataset[:,0:10]
y = dataset[:,10]
X.shape
y.shape

print('Split to train and test')
# Here, the data will be split to train and test. 
# Only 10% of data will be used for testing purpose.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=520)

print('Train a simple neural network')
# Now, let's train a simple neural network 
# with default parameters, just 10 neurons and one hidden layer.
model = NN.MLPClassifier(hidden_layer_sizes=(10,50), random_state=520)
model.fit(X_train, y_train)
y_hat_test = model.predict(X_test)

print('Train score:', model.score(X_train, y_train))
print('Test score:', model.score(X_test, y_test))

print('Confusion matrix')
df = pd.DataFrame({'y_Actual':y, 'y_Predicted':y_hat_test})
confusion_matrix = pd.dataframe(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])
print (confusion_matrix)


# Conclusion: the model is simple and underfits the data

print('Random state')
model = NN.MLPClassifier(hidden_layer_sizes=(10,), random_state=520)
model.fit(X_train, y_train)
print('Train score:', model.score(X_train, y_train))
print('Test score:', model.score(X_test, y_test))

model = NN.MLPClassifier(hidden_layer_sizes=(10,), random_state=521)
model.fit(X_train, y_train)
print('Train score:', model.score(X_train, y_train))
print('Test score:', model.score(X_test, y_test))

model = NN.MLPClassifier(hidden_layer_sizes=(10,), random_state=522)
model.fit(X_train, y_train)
print('Train score:', model.score(X_train, y_train))
print('Test score:', model.score(X_test, y_test))

print('We can see here that result might change depending just on weight initialization.')

print('Scale the data')
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

model = NN.MLPClassifier(hidden_layer_sizes=(10,), random_state=520)
model.fit(X_train_scaled, y_train)
y_hat_test = model.predict(X_test_scaled)
print('Train score:', model.score(X_train_scaled, y_train))
print('Test score:', model.score(X_test_scaled, y_test))


cm = confusion_matrix(y_test, y_hat_test)
cm.print_stats()
cm.plot(backend='seaborn', annot=True, fmt='g')
plt.show()


# # Part 2. Diabetes dataset (regression)

# print('Load the dataset')

# # Here, the data will be split to train and test. 
# # Only 10% of data will be used for testing purpose.
# X, y = datasets.load_diabetes(True)
# print(X.shape)
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=520)

# print('Scale the data')
# scaler = StandardScaler()
# scaler.fit(X_train)
# X_train = scaler.transform(X_train)
# X_test = scaler.transform(X_test)

# # Scale target variable, so we can use logistic activation function
# scaler = MinMaxScaler(feature_range=(0.1, 0.9))
# scaler.fit(y_train.reshape((y_train.shape[0], 1)))
# y_train = scaler.transform(y_train.reshape((y_train.shape[0], 1))).reshape((y_train.shape[0],))
# y_test = scaler.transform(y_test.reshape((y_test.shape[0], 1))).reshape((y_test.shape[0],))

# print('Train a simple neural network')
# model = NN.MLPRegressor(random_state=520)
# model.fit(X_train, y_train)
# y_hat_test = model.predict(X_test)
# y_hat_train = model.predict(X_train)

# print('Train score:', model.score(X_train, y_train))
# print('Test score:', model.score(X_test, y_test))

# def predicted_vs_actual(predicted, actual):
#     plt.plot(predicted, actual, 'ro')
#     plt.xlabel('Predicted')
#     plt.ylabel('Actual')
#     plt.title('Predicted vs Actual')
#     plt.show()
    
# results_plot = predicted_vs_actual

# results_plot(y_hat_train, y_train)
# results_plot(y_hat_test, y_test)

# print('Train more complex neural networks')
# model = NN.MLPRegressor(hidden_layer_sizes = (40, 40, 40, 40), activation='relu', random_state=520, max_iter = 10000, tol=1e-10)
# model.fit(X_train, y_train)
# y_hat_train = model.predict(X_train)
# y_hat_test = model.predict(X_test)
# print('Train score:', model.score(X_train, y_train))
# print('Test score:', model.score(X_test, y_test))
# results_plot(y_hat_train, y_train)
# results_plot(y_hat_test, y_test)

# # What problem do we have? Overfitting!

# # Based on the following:
# # Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html

# import numpy as np
# from sklearn.model_selection import validation_curve

# def plot_validation_curve(estimator, title, X, y, ylim=None, cv=None,
#                           n_jobs=1, iterations=None):
#     plt.figure()
#     plt.title(title)
#     if ylim is not None:
#         plt.ylim(*ylim)
#     plt.xlabel("Number of iterations")
#     plt.ylabel("Score")
#     train_scores, test_scores = validation_curve(model, X, y, "max_iter",
#                                                  iterations,
#                                                  cv=cv)
#     train_scores_mean = np.mean(train_scores, axis=1)
#     train_scores_std = np.std(train_scores, axis=1)
#     test_scores_mean = np.mean(test_scores, axis=1)
#     test_scores_std = np.std(test_scores, axis=1)
#     plt.grid()

#     plt.fill_between(iterations, train_scores_mean - train_scores_std,
#                      train_scores_mean + train_scores_std, alpha=0.1,
#                      color="r")
#     plt.fill_between(iterations, test_scores_mean - test_scores_std,
#                      test_scores_mean + test_scores_std, alpha=0.1, color="g")
#     plt.plot(iterations, train_scores_mean, 'o-', color="r",
#              label="Training score")
#     plt.plot(iterations, test_scores_mean, 'o-', color="g",
#              label="Cross-validation score")

#     plt.legend(loc="best")
#     return plt

# print('Let\'s plot a validation curve')
# model = NN.MLPRegressor(hidden_layer_sizes=(40, 40, 40, 40), activation='relu', random_state=520, tol=1e-10)
# plot_validation_curve(model, "Validation Curve (Neural Network, MLP)", X_train, y_train, ylim=(0.0, 1.01), cv=5, iterations=list(np.arange(5, 150, 3)))
# plt.show()

# print('Let\'s train the optimal number of iterations')
# model = NN.MLPRegressor(hidden_layer_sizes=(40, 40, 40, 40), activation='relu', random_state=520, max_iter=23, tol=1e-10, verbose=True)
# model.fit(X_train, y_train)
# y_hat_train = model.predict(X_train)
# y_hat_test = model.predict(X_test)
# print('Train score:', model.score(X_train, y_train))
# print('Test score:', model.score(X_test, y_test))
# results_plot(y_hat_train, y_train)
# results_plot(y_hat_test, y_test)

# print('Let\'s use cross-validation to find optimal parameters')
# NN_CV = GridSearchCV(NN.MLPRegressor(hidden_layer_sizes=(40, 40, 40, 40), activation='relu', random_state=520, max_iter=10000, tol=1e-10, early_stopping=True),
#                      cv=5,
#                      param_grid={
#                          "alpha": [0.001, 0.1, 1, 10],
#                          "learning_rate_init": [0.001, 0.01, 0.1, 1]
#                      })
# NN_CV.fit(X_train, y_train)
# print('The parameters found by CV search:')
# print(NN_CV.best_params_)
# model = NN_CV
# y_hat_train = model.predict(X_train)
# y_hat_test = model.predict(X_test)
# print('Train score:', model.score(X_train, y_train))
# print('Test score:', model.score(X_test, y_test))
# results_plot(y_hat_train, y_train)
# results_plot(y_hat_test, y_test)

# print('Now, let\'s try logistic activation function')
# NN_CV = GridSearchCV(NN.MLPRegressor(hidden_layer_sizes=(40, 40, 40, 40), activation='logistic', random_state=520, max_iter=10000, early_stopping=True),
#                      cv=5,
#                      param_grid={
#                          "alpha": [0.0001, 0.001, 0.1, 1],
#                          "learning_rate_init": [0.001, 0.01, 0.1, 1]
#                      })
# NN_CV.fit(X_train, y_train)
# print('The parameters found by CV search:')
# print(NN_CV.best_params_)
# model = NN_CV
# y_hat_train = model.predict(X_train)
# y_hat_test = model.predict(X_test)
# print('Train score:', model.score(X_train, y_train))
# print('Test score:', model.score(X_test, y_test))
# results_plot(y_hat_train, y_train)
# results_plot(y_hat_test, y_test)
